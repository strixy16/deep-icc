{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from lifelines.utils import concordance_index\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pysurvival.models.simulations import SimulationModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from utils import *\n",
    "# from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models.py\n",
    "Code from Hassan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    ''' The module class performs building network according to config'''\n",
    "    def __init__(self, activation):\n",
    "        ''' Initialize BasicModel class\n",
    "\n",
    "        Args:\n",
    "            activation: string, name of activation function to use\n",
    "\n",
    "        Returns:\n",
    "            torch.nn Module object, built sequential network\n",
    "        '''\n",
    "        super(BasicModel, self).__init__()\n",
    "        # parses parameters of network from configuration\n",
    "        # Set some defaults for network arguments\n",
    "        # Fraction of input units to drop in dropout layer\n",
    "        self.drop = 0.375#0.401\n",
    "        # Flag to in/exclude normalization layers\n",
    "        self.norm = True\n",
    "        # Default dimensions of fully connected layers\n",
    "        self.dims = [10, 4, 1]#10, 17, 17, 17, 1]\n",
    "        # Activation type to use\n",
    "        self.activation = activation\n",
    "        # Build network using class function (below)\n",
    "        self.model = self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        ''' Performs building networks according to parameters'''\n",
    "        layers = []\n",
    "        for i in range(len(self.dims)-1):\n",
    "            if i and self.drop is not None:\n",
    "                # Add dropout layer\n",
    "                layers.append(nn.Dropout(self.drop))\n",
    "\n",
    "            # Add fully connected layer\n",
    "            layers.append(nn.Linear(self.dims[i], self.dims[i+1]))\n",
    "\n",
    "            if self.norm:\n",
    "                # Add batchnormalize layer\n",
    "                layers.append(nn.BatchNorm1d(self.dims[i+1]))\n",
    "\n",
    "            # Adds activation layer\n",
    "            # eval creates proper format of activation to get from NN\n",
    "            layers.append(eval('nn.{}()'.format(self.activation)))\n",
    "\n",
    "        # Build sequential network from list of layers created in for loop\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Forward propagation through network\n",
    "\n",
    "        Args:\n",
    "            X: data to pass through network\n",
    "\n",
    "        Returns:\n",
    "            Output of model (risk prediction)\n",
    "        '''\n",
    "        return self.model(X)\n",
    "\n",
    "\n",
    "class NegativeLogLikelihood(nn.Module):\n",
    "    '''Negative log likelihood loss function from Katzman et al. (2018) DeepSurv model (equation 4)'''\n",
    "    def __init__(self, gpu):\n",
    "        ''' Initialize NegativeLogLikelihood class\n",
    "\n",
    "        Args:\n",
    "            gpu: string, what kind of tensor to use for loss calculation\n",
    "        '''\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "        # self.L2_reg = 0\n",
    "        self.reg = Regularization(order=2, weight_decay=0)\n",
    "        self.device = gpu\n",
    "\n",
    "    def forward(self, risk_pred, y, e, model):\n",
    "        # Think this is getting set of patients still at risk of failure at time t???\n",
    "        mask = torch.ones(y.shape[0], y.shape[0], device=self.device)\n",
    "        mask[(y.T - y) > 0] = 0\n",
    "        log_loss = torch.exp(risk_pred) * mask\n",
    "        log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "        log_loss = torch.log(log_loss).reshape(-1, 1)\n",
    "        neg_log_loss = -torch.sum((risk_pred-log_loss) * e) / torch.sum(e)\n",
    "        l2_loss = self.reg(model)\n",
    "        return neg_log_loss + l2_loss\n",
    "\n",
    "\n",
    "class NegativeLogLikelihoodStrat(nn.Module):\n",
    "    def __init__(self, gpu):\n",
    "        super(NegativeLogLikelihoodStrat, self).__init__()\n",
    "        self.device = gpu\n",
    "\n",
    "    def forward(self, risk_pred, y, e, low, high):\n",
    "        mask = torch.ones(y.shape[0], y.shape[0], device=self.device)\n",
    "        mask[(y.T - y) > 0] = 0\n",
    "        log_loss = torch.exp(risk_pred) * mask\n",
    "        log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "        log_loss = torch.log(log_loss).reshape(-1, 1)\n",
    "        neg_log_loss = -torch.sum((risk_pred-log_loss) * e) / torch.sum(e)\n",
    "        strat_loss = 1 / (1 + torch.abs((high.mean() - low.mean())))\n",
    "        strat_loss = F.smooth_l1_loss(strat_loss, torch.zeros(1).squeeze().to(self.device), reduction='none').to(self.device)\n",
    "        return neg_log_loss, strat_loss\n",
    "\n",
    "\n",
    "class Regularization(object):\n",
    "    def __init__(self, order, weight_decay):\n",
    "        ''' Initialize Regularization class\n",
    "\n",
    "        Args:\n",
    "            order: int, norm order number\n",
    "            weight_decay: float, weight decay rate\n",
    "        '''\n",
    "        super(Regularization, self).__init__()\n",
    "        self.order = order\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def __call__(self, model):\n",
    "        ''' Calculates regularization(self.order) loss for model\n",
    "\n",
    "        Args:\n",
    "            model: torch.nn Module object\n",
    "\n",
    "        Returns:\n",
    "            reg_loss: torch.Tensor, regularization loss\n",
    "        '''\n",
    "        reg_loss = 0\n",
    "        for name, w in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(w, p=self.order)\n",
    "        reg_loss = self.weight_decay * reg_loss\n",
    "        return reg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py\n",
    "Code from Hassan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurvivalDataset(Dataset):\n",
    "    def __init__(self, dataset, args):\n",
    "        '''Initialize SurvivalDataset class\n",
    "\n",
    "        Args:\n",
    "            dataset: pandas.Dataframe, Contains covariates, time of event (T), and event indicator (E) values.\n",
    "            T and E must be the final two columns\n",
    "            args: Namespace,\n",
    "        '''\n",
    "        # Get covariates out of dataframe (args.covariates is num of columns containing covariates)\n",
    "        self.X = dataset.iloc[:, 0:args.covariates].values\n",
    "        # Get time and event indicator columns out of dataframe\n",
    "        self.data = list(zip(dataset.time, dataset.event))\n",
    "        self.len = len(dataset)\n",
    "        # Normalize covariate data with class function\n",
    "        print('=> load {} samples'.format(self.len))\n",
    "        self._normalize()\n",
    "\n",
    "    def _normalize(self):\n",
    "        '''Normalize X data (covariates) (transform values to range between 0 and 1)'''\n",
    "        self.X = (self.X - self.X.min(axis=0)) / (self.X.max(axis=0) - self.X.min(axis=0))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        '''Getter for single data piece\n",
    "\n",
    "        Args:\n",
    "            item: int, index of data to retrieve\n",
    "\n",
    "        Returns:\n",
    "            X_tensor: torch.Tensor, covariate values for data item\n",
    "            y_tensor: torch.Tensor, time of event value for data item\n",
    "            e_tensor: int torch.Tensor, event indicator value for data item\n",
    "        '''\n",
    "        y, e = self.data[item]\n",
    "        X_tensor = torch.from_numpy(self.X[item])\n",
    "        e_tensor = torch.Tensor([e]).int()\n",
    "        y_tensor = torch.Tensor([y])\n",
    "        return X_tensor, y_tensor, e_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def save_error(train_ci, val_ci, coxLoss, stratLoss, variance, epoch, slname):\n",
    "    '''Save training and validation statistics to csv file\n",
    "\n",
    "        Args:\n",
    "            train_ci: float, training concordance index for this epoch\n",
    "            val_ci: float, validation concordance index for this epoch\n",
    "            coxLoss:\n",
    "            stratLoss:\n",
    "            variance:\n",
    "            epoch: int, epoch these stats are from\n",
    "            slname: string, filename\n",
    "    '''\n",
    "    if epoch == 0:\n",
    "        # Create file for first epoch\n",
    "        f = open(slname, 'w')\n",
    "        f.write('epoch,coxLoss,stratLoss,trainCI,valCI,variance\\n')\n",
    "        f.write('{},{:.4f},{:.4f},{:.4f},{:.4f},{}\\n'.format(epoch, coxLoss, stratLoss, train_ci, val_ci, variance))\n",
    "        f.close()\n",
    "    else:\n",
    "        f = open(slname, 'a')\n",
    "        f.write('{},{:.4f},{:.4f},{:.4f},{:.4f},{}\\n'.format(epoch, coxLoss, stratLoss, train_ci, val_ci, variance))\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def c_index(risk_pred, y, e):\n",
    "    '''Calculate c-index\n",
    "\n",
    "    Args:\n",
    "        risk_pred: np.ndarray or torch.Tensor, model prediction\n",
    "        y: np.ndarray or torch.Tensor, times of event e\n",
    "        e: np.ndarray or torch.Tensor, event indicator\n",
    "\n",
    "    Returns:\n",
    "        c_index: float, concordance index\n",
    "    '''\n",
    "    # Convert risk_pred, y, and e from torch.Tensor to np.ndarray if not already\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = y.detach().cpu().numpy()\n",
    "    if not isinstance(risk_pred, np.ndarray):\n",
    "        risk_pred = risk_pred.detach().cpu().numpy()\n",
    "    if not isinstance(e, np.ndarray):\n",
    "        e = e.detach().cpu().numpy()\n",
    "    return concordance_index(y, risk_pred, e)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, lr_decay_rate):\n",
    "    '''Adjust learning rate according to (epoch, lr, and lr_decay_rate)\n",
    "\n",
    "    Args:\n",
    "        optimizer: torch.optim object,\n",
    "        epoch: int, epoch number\n",
    "        lr: float, initial learning rate\n",
    "        lr_decay_rate: float, decay rate to apply to learning rate\n",
    "\n",
    "    Returns:\n",
    "        lr: float, updated learning rate\n",
    "    '''\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr / (1+epoch*lr_decay_rate)\n",
    "    return optimizer.param_groups[0]['lr']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py\n",
    "Code from Hassan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for network\n",
    "args = Namespace(activation = 'SELU',\n",
    "                 batch_size = 4000,\n",
    "                 covariates = 10, \n",
    "                 decay_interval = 400,\n",
    "                 development = 0,\n",
    "                 dropout = 0.3,\n",
    "                 epochs = 500,\n",
    "                 lib = '',\n",
    "                 lr = 0.001,\n",
    "                 out = 1,\n",
    "                 strat = 'none',# not sure if I actually need this one\n",
    "                 weight_decay = 0.0001\n",
    "                )\n",
    "\n",
    "best_acc = 0\n",
    "# Where to allocate all the Tensors (can be 'cpu' or 'coda')\n",
    "gpu = torch.device(\"cpu\")\n",
    "\n",
    "# Setting up output path from model training\n",
    "root_output = '/Users/katyscott/Documents/ICC/Code/cox_experiments'\n",
    "\n",
    "if args.development == 1:\n",
    "    save_path = 'test'\n",
    "else:\n",
    "    save_path = '{}_{}lr_{}b_'.format(args.activation,args.lr,args.batch_size)\n",
    "    \n",
    "out_dir = os.path.join(root_output, save_path)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Build network \n",
    "model = BasicModel(args.activation).to(gpu)\n",
    "\n",
    "# Loss function\n",
    "criterion = NegativeLogLikelihood(gpu)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data-points: 4000 - Number of events: 3104.0\n",
      "Number of data-points: 500 - Number of events: 392.0\n",
      "=> load 4000 samples\n",
      "=> load 500 samples\n"
     ]
    }
   ],
   "source": [
    "# generate random survival times with exp. distribution\n",
    "sim = SimulationModel(survival_distribution='exponential',\n",
    "                      risk_type = 'Linear',\n",
    "                      censored_parameter = 6,\n",
    "                      alpha = 1,\n",
    "                      beta = 5)\n",
    "\n",
    "train_samples = sim.generate_data(num_samples = 4000,\n",
    "                                  num_features = args.covariates,\n",
    "                                  feature_weights = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "val_samples = sim.generate_data(num_samples = 500,\n",
    "                                num_features = args.covariates,\n",
    "                                feature_weights = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "train_dataset = SurvivalDataset(train_samples, args)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_dataset.__len__())\n",
    "\n",
    "val_dataset = SurvivalDataset(val_samples, args)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.206840\n",
       "1       1.027174\n",
       "2       1.718268\n",
       "3       2.283229\n",
       "4       3.595767\n",
       "          ...   \n",
       "3995    1.304077\n",
       "3996    0.159206\n",
       "3997    0.076456\n",
       "3998    0.419792\n",
       "3999    1.888246\n",
       "Name: time, Length: 4000, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and setup cholangio genetic data\n",
    "genomic_file = \"../Data/MSK_Genomic_Data.csv\"\n",
    "\n",
    "gene_features = pd.read_csv(genomic_file)\n",
    "#Patient IDs have a space at the end of the name\n",
    "gene_features['ScoutID'] = gene_features['ScoutID'].str.strip()\n",
    "# Fixing columns with illegal characters in name\n",
    "gene_features.rename(columns={'CDKN2A.DEL':'CDKN2A_DEL', 'TGF-Beta_Pathway':'TGF_Beta_Pathway'}, inplace=True)\n",
    "\n",
    "# Get number of covariates = number of genetic columns\n",
    "args.covariates = gene_features.shape[1] - 1\n",
    "\n",
    "labels_file = \"../Data/RFS_Scout.xlsx\"\n",
    "\n",
    "rfs_labels = pd.read_excel(labels_file)\n",
    "rfs_labels = rfs_labels[['ScoutID', 'RFS', 'RFS_Code']]\n",
    "rfs_labels.rename(columns={'RFS':'time', 'RFS_Code':'event'}, inplace=True)\n",
    "\n",
    "# Getting intersection of patients with gene features and RFS labels all in one dataframe\n",
    "genes_and_labels = pd.merge(gene_features, rfs_labels, how='inner', on=['ScoutID', 'ScoutID'])\n",
    "\n",
    "# Removing ScoutID so setup is proper for Survival Dataset generation\n",
    "genes_and_labels.drop(columns=['ScoutID'], inplace=True)\n",
    "\n",
    "train_genes, val_genes = train_test_split(genes_and_labels, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> load 88 samples\n",
      "=> load 23 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-247bc64a61da>:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.X = (self.X - self.X.min(axis=0)) / (self.X.max(axis=0) - self.X.min(axis=0))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SurvivalDataset(train_genes, args)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_dataset.__len__())\n",
    "\n",
    "val_dataset = SurvivalDataset(val_genes, args)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (88x18 and 10x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-410e08ac6d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Get risk prediction from network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrisk_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Calculate neg. log likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-da945ef611bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mOutput\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrisk\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         '''\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (88x18 and 10x4)"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    coxLossMeter = AverageMeter()\n",
    "    stratLossMeter = AverageMeter()\n",
    "    ciMeter = AverageMeter()\n",
    "    varMeter = AverageMeter()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for X, y, e in train_loader:\n",
    "        # Get risk prediction from network\n",
    "        risk_pred = model(X.float().to(gpu))\n",
    "        \n",
    "        # Calculate neg. log likelihood\n",
    "        cox_loss = criterion(-risk_pred, y.to(gpu), e.to(gpu), model)\n",
    "        strat_loss = torch.Tensor([0])\n",
    "        train_loss = cox_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        coxLossMeter.update(cox_loss.item(), y.size(0))\n",
    "        stratLossMeter.update(strat_loss.item(), y.size(0))\n",
    "        varMeter.update(risk_pred.var(), y.size(0))\n",
    "        \n",
    "        # Calculate c index\n",
    "        train_c = c_index(risk_pred, y, e)\n",
    "        ciMeter.update(train_c.item(), y.size(0))\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    ciValMeter = AverageMeter()\n",
    "    for X, y, e in val_loader:\n",
    "        risk_pred = model(X.float().to(gpu))\n",
    "        val_c = c_index(risk_pred, y, e)\n",
    "        ciValMeter.update(val_c.item(), y.size(0))\n",
    "    \n",
    "    print('Epoch: {} \\t Train Loss: {:.4f} \\t Train CI: {:.3f} \\t Val CI: {:.3f}'.format(epoch, train_loss, train_c, val_c))\n",
    "    save_error(ciMeter.avg, ciValMeter.avg, coxLossMeter.avg, stratLossMeter.avg, varMeter.avg, epoch, os.path.join(out_dir, 'convergence.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepicc",
   "language": "python",
   "name": "deepicc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
