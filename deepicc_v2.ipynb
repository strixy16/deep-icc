{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from lifelines.utils import concordance_index\n",
    "import numpy as np\n",
    "import os\n",
    "from pysurvival.models.simulations import SimulationModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models.py\n",
    "Code from Hassan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicModel(nn.Module):\n",
    "#     '''Module class builds network according to config'''\n",
    "#     def __init__(self, activation='SELU'):\n",
    "#         ''' Initialize BasicModel class\n",
    "        \n",
    "#         Args:\n",
    "#             activation: string, name of activation function to use\n",
    "        \n",
    "#         Returns:\n",
    "#             torch.nn Module object, built sequential network\n",
    "#         '''\n",
    "#         super(BasicModel, self).__init__()\n",
    "        \n",
    "#         # Set some defaults for network arguments\n",
    "#         # Fraction of input units to drop in dropout layer\n",
    "#         self.drop = 0.375\n",
    "#         # Flag to in/exclude normalization layer\n",
    "#         self.norm = True\n",
    "#         # Default dimensions of fully connected layers\n",
    "#         self.dims = [10, 4, 1]\n",
    "#         # Activation type to use\n",
    "#         self.activation = activation\n",
    "        \n",
    "#         # Build network using class function (below)\n",
    "#         self.model = self._build_network()\n",
    "    \n",
    "#     def _build_network(self):\n",
    "#         '''Build network according to parameters'''\n",
    "#         layers =  []\n",
    "        \n",
    "#         for i in range(len(self.dims)-1):\n",
    "#             if i and self.drop is not None:\n",
    "#                 # Add dropout layer\n",
    "#                 layers.append(nn.Dropout(self.drop))\n",
    "            \n",
    "#             # Add fully connected layer\n",
    "#             layers.append(nn.Linear(self.dims[i], self.dims[i+1]))\n",
    "            \n",
    "#             if self.norm:\n",
    "#                 # Add batch normalize layer\n",
    "#                 layers.append(nn.BatchNorm1d(self.dims[i+1]))\n",
    "                \n",
    "#             # Add activation layer\n",
    "#             # eval creates the proper format of the activation to get from nn\n",
    "#             layers.append(eval('nn.{}()'.format(self.activation)))\n",
    "            \n",
    "#         # Build sequential network from list of layers created in for loop\n",
    "#         return nn.Sequential(*layers)\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         ''' Forward propagation through network\n",
    "        \n",
    "#         Args:\n",
    "#             X: data to pass through network\n",
    "        \n",
    "#         Returns:\n",
    "#             Output of model (risk prediction)\n",
    "#         '''\n",
    "#         return self.model(X)\n",
    "    \n",
    "\n",
    "# class NegativeLogLikelihood(nn.Module):\n",
    "#     '''Negative log likelihood loss function from Katzman et al. (2018) DeepSurv model (equation 4)'''\n",
    "    \n",
    "#     def __init__(self, gpu='None'):\n",
    "#         ''' Initialize NegativeLogLikelihood class\n",
    "        \n",
    "#         Args:\n",
    "#             gpu: string, what kind of tensor to use for loss calculation \n",
    "#         '''\n",
    "#         super(NegativeLogLikelihood, self).__init__()\n",
    "# #         self.L2_reg = 0\n",
    "#         self.reg = Regularization(order=2, weight_decay=0)\n",
    "#         self.device = gpu\n",
    "        \n",
    "#     def forward(self, risk_pred, y, e, model):\n",
    "#         # Think this is getting set of patients still at risk of failure at time t???\n",
    "#         mask = torch.ones(y.shape[0], y.shape[0], device=self.device)\n",
    "#         mask[(y.T - y) > 0] = 0\n",
    "        \n",
    "#         log_loss = torch.exp(risk_pred) * mask\n",
    "#         log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "#         log_loss = torch.log(log_loss).reshape(-1,1)\n",
    "        \n",
    "#         neg_log_loss = -torch.sum((risk_pred - log_loss) * e) / torch.sum(e)\n",
    "        \n",
    "#         l2_loss = self.reg(model)\n",
    "        \n",
    "#         return neg_log_loss + l2_loss\n",
    "    \n",
    "\n",
    "# class Regularization(object):\n",
    "#     def __init__(self, order, weight_decay):\n",
    "#         ''' Initialize Regularization class\n",
    "        \n",
    "#         Args:\n",
    "#             order: int, norm order number\n",
    "#             weight_decay: float, weight decay rate\n",
    "#         '''\n",
    "#         super(Regularization, self).__init__()\n",
    "#         self.order = order\n",
    "#         self.weight_decay = weight_decay\n",
    "        \n",
    "#     def __call__(self, model):\n",
    "#         ''' Calculates regularization(self.order) loss for model\n",
    "        \n",
    "#         Args:\n",
    "#             model: torch.nn Module object\n",
    "        \n",
    "#         Returns:\n",
    "#             reg_loss: torch.Tensor, regularization loss\n",
    "#         '''\n",
    "#         reg_loss = 0\n",
    "#         for name, w in model.named_parameters():\n",
    "#             if 'weight' in name:\n",
    "#                 reg_loss = reg_loss + torch.norm(w, p=self.order)\n",
    "            \n",
    "#             reg_loss = self.weight_decay * reg_loss\n",
    "#             return reg_loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py\n",
    "Code from Hassan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SurvivalDataset(Dataset):\n",
    "#     def __init__(self, dataset, args):\n",
    "#         '''Initialize SurvivalDataset class\n",
    "        \n",
    "#         Args:\n",
    "#             dataset: pandas.Dataframe, Contains covariates, time of event (T), and event indicator (E) values.\n",
    "#             T and E must be the final two columns\n",
    "#             args: Namespace, \n",
    "#         '''\n",
    "#         # Get covariates out of dataframe (args.covariates is num of columns containing covariates)\n",
    "#         self.X = dataset.iloc[:, 0:args.covariates].values\n",
    "#         # Get time and event indicator columns out of dataframe\n",
    "#         self.data = list(zip(dataset.time, dataset.event))\n",
    "#         self.len = len(dataset)\n",
    "#         print('=> load {} samples'.format(self.len))\n",
    "#         # Normalize covariate data with class function\n",
    "#         self._normalize()\n",
    "        \n",
    "#     def _normalize(self):\n",
    "#         '''Normalize X data (covariates) (transform values to range between 0 and 1)'''\n",
    "#         self.X = (self.X - self.X.min(axis=0)) / (self.X.max(axis=0) - self.X.min(axis=0))\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         '''Getter for single data piece\n",
    "\n",
    "#         Args:\n",
    "#             item: int, index of data to retrieve\n",
    "\n",
    "#         Returns:\n",
    "#             X_tensor: torch.Tensor, covariate values for data item\n",
    "#             y_tensor: torch.Tensor, time of event value for data item\n",
    "#             e_tensor: int torch.Tensor, event indicator value for data item\n",
    "#         '''\n",
    "#         y, e = self.data[item]\n",
    "#         X_tensor = torch.from_numpy(self.X[item])\n",
    "#         y_tensor = torch.Tensor([y])\n",
    "#         e_tensor = torch.Tensor([e]).int\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "        \n",
    "\n",
    "# def save_error(train_ci, val_ci, coxLoss, stratLoss, variance, epoch, slname):\n",
    "#     '''Save training and validation statistics to csv file\n",
    "    \n",
    "#     Args:\n",
    "#         train_ci: float, training concordance index for this epoch\n",
    "#         val_ci: float, validation concordance index for this epoch\n",
    "#         coxLoss: \n",
    "#         stratLoss:\n",
    "#         variance:\n",
    "#         epoch: int, epoch these stats are from\n",
    "#         slname: string, filename \n",
    "#     '''\n",
    "#     if epoch == 0:\n",
    "#         # Create file for first epoch\n",
    "#         f = open(slname, 'w')\n",
    "#         f.write('epoch,coxLoss,stratLoss,trainCI,valCI,variance\\n')\n",
    "#         f.write('{},{:.4f},{:.4f},{:.4f},{:.4f},{}\\n'.format(epoch, coxLoss, stratLoss, train_ci, val_ci, variance))\n",
    "#         f.close()\n",
    "#     else:\n",
    "#         f = open(slname, 'a')\n",
    "#         f.write('{},{:.4f},{:.4f},{:.4f},{:.4f},{}\\n'.format(epoch, coxLoss, stratLoss, train_ci, val_ci, variance))\n",
    "#         f.close()\n",
    "\n",
    "\n",
    "# def c_index(risk_pred, y, e):\n",
    "#     '''Calculate c-index\n",
    "    \n",
    "#     Args:\n",
    "#         risk_pred: np.ndarray or torch.Tensor, model prediction\n",
    "#         y: np.ndarray or torch.Tensor, times of event e\n",
    "#         e: np.ndarray or torch.Tensor, event indicator\n",
    "    \n",
    "#     Returns:\n",
    "#         c_index: float, concordance index \n",
    "#     '''\n",
    "#     # Convert risk_pred, y, and e from torch.Tensor to np.ndarray if not already\n",
    "#     if not isinstance(risk_pred, np.ndarray):\n",
    "#         risk_pred = risk_pred.detach().cpu().numpy()\n",
    "        \n",
    "#     if not isinstance(y, np.ndarray):\n",
    "#         y = y.detach().cpu().numpy()\n",
    "        \n",
    "#     if not isinstance(e, np.ndarray):\n",
    "#         e = e.detach().cpu().numpy()\n",
    "    \n",
    "#     return concordance_index(y, risk_pred, e)\n",
    "\n",
    "\n",
    "# class AverageMeter(object):\n",
    "#     '''Computes and stores average and current value'''\n",
    "#     def __init__(self):\n",
    "#         self.reset()\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.val = 0\n",
    "#         self.avg = 0\n",
    "#         self.sum = 0\n",
    "#         self.count = 0\n",
    "        \n",
    "#     def update(self, val, n=1):\n",
    "#         self.val = val\n",
    "#         self.sum += val * n\n",
    "#         self.count += n\n",
    "#         self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "# def adjust_learning_rate(optimizer, epoch, lr, lr_decay):\n",
    "#     '''Adjust learning rate according to (epoch, lr, and lr_decay_rate)\n",
    "    \n",
    "#     Args:\n",
    "#         optimizer: torch.optim object, \n",
    "#         epoch: int, epoch number\n",
    "#         lr: float, initial learning rate \n",
    "#         lr_decay_rate: float, decay rate to apply to learning rate\n",
    "    \n",
    "#     Returns:\n",
    "#         lr: float, updated learning rate\n",
    "#     '''\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr / (1+epoch*lr_decay_rate)\n",
    "    \n",
    "#     return optimizer.param_groups[0]['lr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.py\n",
    "Code from Hassan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for network\n",
    "args = Namespace(activation = 'SELU',\n",
    "                 batch_size = 4000,\n",
    "                 covariates = 10, \n",
    "                 decay_interval = 400,\n",
    "                 development = 0,\n",
    "                 dropout = 0.3,\n",
    "                 epochs = 500,\n",
    "                 lib = '',\n",
    "                 lr = 0.001,\n",
    "                 out = 1,\n",
    "                 strat = 'none',# not sure if I actually need this one\n",
    "                 weight_decay = 0.0001\n",
    "                )\n",
    "\n",
    "best_acc = 0\n",
    "# Where to allocate all the Tensors (can be 'cpu' or 'coda')\n",
    "gpu = torch.device(\"cpu\")\n",
    "\n",
    "# Setting up output path from model training\n",
    "root_output = '/Users/katyscott/Documents/ICC/Code/cox_experiments'\n",
    "\n",
    "if args.development == 1:\n",
    "    save_path = 'test'\n",
    "else:\n",
    "    save_path = '{}_{}lr_{}b_'.format(args.activation,args.lr,args.batch_size)\n",
    "    \n",
    "out_dir = os.path.join(root_output, save_path)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Build network \n",
    "model = BasicModel(args.activation).to(gpu)\n",
    "\n",
    "# Loss function\n",
    "criterion = NegativeLogLikelihood(gpu)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data-points: 4000 - Number of events: 3138.0\n",
      "Number of data-points: 500 - Number of events: 381.0\n",
      "=> load 4000 samples\n",
      "=> load 500 samples\n"
     ]
    }
   ],
   "source": [
    "# generate random survival times with exp. distribution\n",
    "sim = SimulationModel(survival_distribution='exponential',\n",
    "                      risk_type = 'Linear',\n",
    "                      censored_parameter = 6,\n",
    "                      alpha = 1,\n",
    "                      beta = 5)\n",
    "\n",
    "train_samples = sim.generate_data(num_samples = 4000,\n",
    "                                  num_features = args.covariates,\n",
    "                                  feature_weights = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "val_samples = sim.generate_data(num_samples = 500,\n",
    "                                num_features = args.covariates,\n",
    "                                feature_weights = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "train_dataset = SurvivalDataset(train_samples, args)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_dataset.__len__())\n",
    "\n",
    "val_dataset = SurvivalDataset(val_samples, args)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f016ae31c289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "for X, y, e in train_loader:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7b90e44f0101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Get risk prediction from network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrisk_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/deepicc/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, args.epochs):\n",
    "    coxLossMeter = AverageMeter()\n",
    "    stratLossMeter = AverageMeter()\n",
    "    ciMeter = AverageMeter()\n",
    "    varMeter = AverageMeter()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    for X, y, e in train_loader:\n",
    "        # Get risk prediction from network\n",
    "        risk_pred = model(X.float().to(gpu))\n",
    "        \n",
    "        # Calculate neg. log likelihood\n",
    "        cox_loss = criterion(-risk_pred, y.to(gpu), e.to(gpu), low, high)\n",
    "        strat_loss = torch.Tensor([0])\n",
    "        train_loss = cox_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        coxLossMeter.update(cox_loss.item(), y.size(0))\n",
    "        stratLossMeter.update(strat_loss.item(), y.size(0))\n",
    "        varMeter.update(risk_pred.var(), y.size(0))\n",
    "        \n",
    "        # Calculate c index\n",
    "        train_c = c_index(risk_pred, y, e)\n",
    "        ciMeter.update(train_c.item(), y.size(0))\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    ciValMeter = AverageMeter()\n",
    "    for X, y, e in val_loader:\n",
    "        risk_pred = model(X.float().to(gpu))\n",
    "        val_c = c_index(risk_pred, y, e)\n",
    "        ciValMeter.update(val_c.item(), y.size(0))\n",
    "    \n",
    "    print('Epoch: {} \\t Train Loss: {:.4f} \\t Train CI: {:.3f} \\t Val CI: {:.3f}'.format(epoch, train_loss, train_c, val_c))\n",
    "    save_error(ciMeter.avg, ciValMeter.avg, coxLossMeter.avg, stratLossMeter.avg, varMeter.avg, epoch, os.path.join(out_dir, 'convergence.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepicc",
   "language": "python",
   "name": "deepicc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
